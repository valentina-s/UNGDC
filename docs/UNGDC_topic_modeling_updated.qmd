---
title: "Dynamic UNGDC (updated)"
format: html
editor: visual
execute:
  warning: false
---

This version *does* use tf-idf for LDA analysis. For the older version, refer to `UNGDC_topic_modeling.qmd`. I created a separate version for two reasons. First, some of the functions and options deprecated from the `quanteda` R package. Earlier version might not be reproducible. Second, the inclusion of tf-idf to generate LDA analysis has a tradeoff. Since it gives less weights to terms that appear frequently across the documents, by definition, tf-idf lowers the correlation between terms over different time window. It is harder to notice a clear linkage between two topics represented by different terms. However, unlike the earlier version that excludes tf-idf, topics are more specific, and substantively meaningful.

-   TO DO: Reduce the number of topics from 10 to 5. Tf-idf increased the between-document variation and relatively decreased the commonality across the documents. It became harder to find a common topic, and it reducing the number of topics might help. I also need to implement moving window to produce a smoother transition of topics over time.

## Dynamic Topic Modeling for UNGDC

In order to generate LDA topic modeling results for the corpus of UNGD, I split the corpus into different time frames. The entire time span of 1945 until 2022 is split into 8 intervals, with a duration of 10 years.

```{r}
# Load packages
library(plyr)
library(dplyr)
library(tm)
library(gplots)
library(ggplot2)
library(quanteda)
library(readr)
library(seededlda)
library(slam)
library(jsonlite)
library(tm)
library(tidyr)
library(knitr)

light <- readRDS("data/processed/cleaned.RDS")

#Set up the parameters
light_interval <- light %>%
  dplyr::mutate(span = as.factor(cut(year,
                                     breaks = c(seq(from = 1945, to = 2022, by = 10), 2022)))) %>%
  dplyr::arrange(year)


# I added two additional stop words that aren't captured in the generic stop words dictionary. 

mystopwords <- c("will", "must")
custom_stopwords <- c(stopwords("english"), mystopwords)

```

## Term Frequency-Inverse Matrix and Descriptive Data Visualization

-   To inspect the data and frequent words across time intervals, below code generates top-20 terms based on the tf-idf scores.
-   Input dataset: ["data/processed/cleaned.RDS"](https://github.com/Jihyeonbae/UNGDC).

```{r}
#| eval =FALSE

# Function for generating tf_idf and plots.
sapply(levels(light_interval$span), function(i) {
  subset_i <- light_interval %>% dplyr::filter(span %in% i)
  corpus_subset <- Corpus(VectorSource(subset_i$text))
  tdm <- TermDocumentMatrix(corpus_subset,
                            control = list(weighting = weightTfIdf,
                                           removePunctuation = TRUE,
                                           stemming = TRUE,
                                           removeNumbers = TRUE,
                                           stopwords = TRUE,
                                           removewords = mystopwords))
  top_terms <- slam::row_sums(as.matrix(tdm))
  
  # Create a data frame with terms and tfidf values
  top_terms_df <- data.frame(term = names(top_terms), tfidf = top_terms)
  
  # Order the terms by tfidf value
  top_terms_df <- top_terms_df[order(top_terms_df$tfidf, decreasing = TRUE), ]
  
  # Select the top 20 terms
  top_terms_df <- head(top_terms_df, 20)
  
  figure_i <- ggplot(top_terms_df, aes(x = reorder(term, tfidf), y = tfidf)) +
    geom_bar(stat = "identity", fill = "skyblue") +
    theme_minimal() +
    labs(title = "Top 20 Terms by TF-IDF",
         x = "Terms",
         y = "TF-IDF Score") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  output_file <- file.path("figs/", paste0("plot_", i, ".png"))
  ggsave(output_file, figure_i, width = 8, height = 5, units = "in")
})
```

-   dfm function helps remove stop words and perform other preprocessing steps to create a more refined document-feature matrix. Additionally, the subsequent dfm_tfidf function is used to compute TF-IDF (Term Frequency-Inverse Document Frequency) scores, which down-weights terms that appear frequently across documents.

# LDA Topic Modeling

-   In this section, I perform topic modeling using LDA, an unsupervised method to estimate vectors that represent topics within a speech.It assumes that each speech document is a mixture of topics, and each word is attributable to one of the document's topics.

-   Current parameters in the below code sets the number of topics as 10, and uses the TF-IDF scores.

-   In order *not* to replicate the entire process of LDA modeling, download this folder that contains lda outputs here `` [`output/lda/decade_0120_replicate` ``\](<https://drive.google.com/drive/folders/1woSqpayZGCozaFe1NoZuEYzdx7zv32wr?usp=share_link>).

# LDA Generator Updated Function

```{r}

## New version since dfm() options deprecated within the quanteda package.
lda_generator <- function(corpus, span_levels, num_topics = 10) {
  if (!dir.exists(output_dir)) {
    dir.create(output_dir, recursive = TRUE)
  }

  for (i in span_levels) {
    subset <- corpus[corpus$span == i, ]
    
    # Use tokens() instead of dfm() for tokenization
    subset_tokens <- tokens(subset$text,
                            remove_numbers = TRUE,
                            remove_punct = TRUE,
                            split_hyphens = TRUE) %>%
                    tokens_wordstem() %>%
                    tokens_tolower() %>%
                    tokens_remove(pattern = custom_stopwords) %>%
                    tokens_compound(pattern = phrase(c("human rights*", "united nations")))
    
    # Apply TF-IDF transformation directly on tokens
    subset_tfidf <- dfm(subset_tokens) %>%
                     dfm_tfidf(scheme_tf = "count", scheme_df = "inverse")

     timing <- system.time({
      dfm_output_file <- file.path(output_dir, paste0("dfm_", i, ".RDS"))
      saveRDS(convert(subset_tfidf, to="data.frame"), dfm_output_file)

      tmod_lda <- textmodel_lda(subset_tfidf, k = num_topics)
      lda_output_file <- file.path(output_dir, paste0("lda_model_", i, ".RDS"))
      saveRDS(tmod_lda, lda_output_file)
    })

    cat(sprintf("Time taken for %s: %s seconds\n", i, timing[3]))
  }
}
  

# Loop through each decade and save separate LDA results. 
span_levels <- levels(light_interval$span)
#lda_generator(light_interval, span_levels)

```

## Reading in LDA results

After running the LDA model, I read in each LDA results as a separate element in a list form. Below code prins out top 10 terms associated with each topic in the LDA models for different span levels. Each row represents one semantic topic.

```{r}


read_lda_models <- function(span_levels, output_dir = "output/lda/decade_0120_replicate/") {
  lda_models <- list()

  for (i in span_levels) {
    lda_output_file <- file.path(output_dir, paste0("lda_model_", i, ".RDS"))
    
    print(lda_output_file)

    if (file.exists(lda_output_file)) {
      lda_model <- readRDS(lda_output_file)
      lda_models[[i]] <- lda_model
      cat(sprintf("LDA model for %s successfully loaded.\n", i))
    } else {
      cat(sprintf("LDA model file for %s not found.\n", i))
    }
  }

  return(lda_models)
}



lda_models <- read_lda_models(span_levels)

topic_tables <- function(lda_models, span_levels) {
  topic_tables <- list()

  for (i in span_levels) {
    if (i %in% names(lda_models)) {
      lda_model <- lda_models[[i]]
      terms <- terms(lda_model, 10)
      topic_table <- data.frame(Terms = terms)
      topic_tables[[i]] <- topic_table
    } else {
      cat(sprintf("LDA model for %s not found.\n", i))
    }
  }

  all_topics <- do.call(rbind, topic_tables)
  return(all_topics)
}


topic_tables <- topic_tables(lda_models, span_levels)
print(knitr::kable(topic_tables))
```

-   Each column in the dataset corresponds to a vector of terms representing a specific topic. However, extracting substantively meaningful topics poses challenges due to several issues. One notable challenge is the variability in the set of terms used to represent the same topic across different time periods. For instance, the topic of international security may be discussed in relation to the Soviet Union and North Korea in earlier time periods, whereas in more recent times, it may be associated with Russia and Ukraine.

-   Another important problem is identifying related topics over time. There is a difficulty of establishing connections between topics and understanding their evolution across different temporal contexts. Some topics and terms disappear abruptly, while new topics emerge. Identifying the connection between vectors poses a challenge.

## Dynamic Topic Modeling

-   To address the above mentioned challenges, we refered to existing papers.
-   `"BERTopic Dynamic Topic Modeling"`(https://maartengr.github.io/BERTopic/getting_started/topicsovertime/topicsovertime.html)
-   Greene and cross, 2017 (https://doi.org/10.1017/pan.2016.7)

### This generates output for a single pair of time frames

```{r}
#| eval = FALSE
model1<-lda_models[[1]]
model2<-lda_models[[2]]

# phi value is a topic probability of every word
phi1 <- model1$phi

#phi1$topic <- sequence(nrow(phi1))

phi2 <- model2$phi
#phi2$topic <- sequence(nrow(phi2))


# Convert matrices to data frames
phi1_df <- as.data.frame(phi1)
phi2_df <- as.data.frame(phi2)

order_phi1 <- order(colMeans(phi1_df), decreasing = TRUE)
order_phi2 <- order(colMeans(phi2_df), decreasing = TRUE)

# Reorder columns based on the mean
phi1_df <- phi1_df[, order_phi1]
phi2_df <- phi2_df[, order_phi2]

# Identify columns to drop based on colMeans
## Try without dropping
columns_to_drop_phi1 <- colMeans(phi1_df) < 0.00001
columns_to_drop_phi2 <- colMeans(phi2_df) < 0.00001

# Drop identified columns
phi1_df <- phi1_df[, !columns_to_drop_phi1, drop = FALSE]
phi2_df <- phi2_df[, !columns_to_drop_phi2, drop = FALSE]


# Get the union of column names
all_terms <- union(colnames(phi1_df), colnames(phi2_df))

#fill missing values with zeros
phi1_union <- bind_cols(phi1_df, setNames(data.frame(matrix(0, nrow = nrow(phi1_df), ncol = length(setdiff(all_terms, colnames(phi1_df))))), setdiff(all_terms, colnames(phi1_df))))
phi2_union <- bind_cols(phi2_df, setNames(data.frame(matrix(0, nrow = nrow(phi2_df), ncol = length(setdiff(all_terms, colnames(phi2_df))))), setdiff(all_terms, colnames(phi2_df))))

# Reorder columns alphabetically
phi1_union <- phi1_union[, order(colnames(phi1_union))]
phi2_union <- phi2_union[, order(colnames(phi2_union))]


dim(phi1_union)
dim(phi2_union)


cor<-cor(t(phi1_union), t(phi2_union))


heatmap.2(cor,
          Rowv = FALSE, Colv = FALSE,
          col = heat.colors(256),
          trace = "none", # no row/column names
          key = TRUE, keysize = 1.5,
          density.info = "none", margins = c(5, 5),
          cexCol = 1, cexRow = 1, # adjust text size
          notecol = "black", notecex = 0.7,
          main = "Correlation Matrix",
          xlab = "Period 2", ylab = "Period 1",
          symkey = FALSE)

order_phi1_union <- order(colMeans(phi1_union), decreasing = TRUE)
phi1_result <- phi1_union[, order_phi1_union]

order_phi2_union <- order(colMeans(phi2_union), decreasing = TRUE)
phi2_result <- phi2_union[, order_phi2_union]


phi1_result_row <- orderBasedOnRow(phi1_union, 1)
phi1_result_long<-phi1_result_row%>%
  tidyr::pivot_longer(everything(), names_to="term_1", values_to="probability_1")

phi2_result_row <- orderBasedOnRow(phi2_union, 6)
phi2_result_long<-phi2_result_row%>%
  tidyr::pivot_longer(everything(), names_to="term_2", values_to="probability_2")

pair<-bind_cols(phi1_result_long, phi2_result_long)

```

#Function to print out the words

```{r}
orderBasedOnRow <- function(df, I) {
  # Order columns based on the Ith row values
  ordered_cols <- order(apply(df, 2, function(x) x[I]), decreasing = TRUE)

  # Reorder the data frame columns
  ordered_df <- df[, ordered_cols]

  ordered_row <- ordered_df[I, 1:10]

  return(ordered_row)
}

```

## Below function generates heatmaps for a pair of models.

```{r}
generate_heatmap <- function(model1, model2, correlation_threshold = 0.9) {
  phi1 <- model1$phi
  phi2 <- model2$phi

  phi1_df <- as.data.frame(phi1)
  phi2_df <- as.data.frame(phi2)
  
  all_terms <- union(colnames(phi1_df), colnames(phi2_df))

  phi1_union <- bind_cols(phi1_df, setNames(data.frame(matrix(0, nrow = nrow(phi1_df), ncol = length(setdiff(all_terms, colnames(phi1_df))))), setdiff(all_terms, colnames(phi1_df))))
  phi2_union <- bind_cols(phi2_df, setNames(data.frame(matrix(0, nrow = nrow(phi2_df), ncol = length(setdiff(all_terms, colnames(phi2_df))))), setdiff(all_terms, colnames(phi2_df))))

  phi1_union <- phi1_union[, order(colnames(phi1_union))]
  phi2_union <- phi2_union[, order(colnames(phi2_union))]

  dim(phi1_union)
  dim(phi2_union)

  cor_matrix <- cor(t(phi1_union), t(phi2_union))

  # Heatmap for correlation matrix
  heatmap.2(cor_matrix,
            Rowv = FALSE, Colv = FALSE,
            col = heat.colors(16),
            trace = "none", # no row/column names
            key = TRUE, keysize = 1.5,
            density.info = "none", margins = c(5, 5),
            cexCol = 1, cexRow = 1, # adjust text size
            notecol = "black", notecex = 0.7,
            xlab = "Time 2",
            ylab = "Time 1",
            symkey = FALSE)

  return(list(phi1_union = phi1_union, phi2_union = phi2_union, cor_matrix = cor_matrix))
}

```

## Rows with high correlation

```{r}
# Function to print the ordered rows for each topic with high correlation
print_ordered_rows <- function(phi1_union, phi2_union, cor_matrix, high_corr_indices, correlation_threshold = 0.9) {
  # Find indices where correlation is higher than the threshold
  high_corr_indices <- which(cor_matrix > correlation_threshold & !is.na(cor_matrix), arr.ind = TRUE)

  # Create an empty list to store results
  result_list <- list()

  # Print the ordered rows for each topic with high correlation
  for (i in seq_len(nrow(high_corr_indices))) {
    model1_topic <- high_corr_indices[i, 1]
    model2_topic <- high_corr_indices[i, 2]

    # Print the ordered rows for each model's topic
    cat(paste("Model 1 - Topic", model1_topic), "\n")
    phi1_result_row <- orderBasedOnRow(phi1_union, model1_topic)

    cat(paste("Model 2 - Topic", model2_topic), "\n")
    phi2_result_row <- orderBasedOnRow(phi2_union, model2_topic)

    # Convert result rows to long format
    phi1_result_long <- phi1_result_row %>%
      tidyr::pivot_longer(everything(), names_to = "term_1", values_to = "probability_1")

    phi2_result_long <- phi2_result_row %>%
      tidyr::pivot_longer(everything(), names_to = "term_2", values_to = "probability_2")

    # Combine phi1 and phi2 results
    pair <- bind_cols(phi1_result_long, phi2_result_long)

    # Append the result to the list
    result_list[[i]] <- pair
  }

  # Combine all results into a single dataframe
  final_result <- do.call(bind_rows, result_list)

  return(final_result)
}

```

## Execute functions over pairs

```{r}
# Loop through pairs of models to generate heatmaps and print results
for (i in 1:(length(lda_models) - 1)) {
  model1 <- lda_models[[i]]
  model2 <- lda_models[[i + 1]]

  result <- generate_heatmap(model1, model2, correlation_threshold = 0.5)
  
  phi1_union <- result$phi1_union
  phi2_union <- result$phi2_union
  cor_matrix <- result$cor_matrix

  # Print ordered rows only if there are high correlations
  if (any(cor_matrix > 0.5, na.rm = TRUE)) {
    phi1_result <- phi1_union[, order(colMeans(phi1_union), decreasing = TRUE)]
    phi2_result <- phi2_union[, order(colMeans(phi2_union), decreasing = TRUE)]

    # Call the modified function and pass high_corr_indices as an argument
    final_result <- print_ordered_rows(phi1_result, phi2_result, cor_matrix, high_corr_indices, correlation_threshold = 0.5)
    print(final_result)
  }
}


```

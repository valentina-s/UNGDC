{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "846741df-0991-4c18-ad55-c423db524254",
   "metadata": {},
   "source": [
    "# BERT Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fc61d04-250d-4b07-a700-6a3820a20878",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "be8c2b8e-cfa0-4af1-8b33-2861320258b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b006215f-3db4-4611-b977-b0d70362f50b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It privilege express , Mr. President, congratulations Afghanistan delegation election, justly unanimously voted Assembly. It also privilege extend fellow representatives greetings Royal Afghan Government, well sincerest wishes success current session General Assembly.Our attachment United Nations Charter principles complete adherence principles human rights self-determination peoples based ideological grounds also result long experience free small country controversial events modern history. We believe peace world can secured bases, certain future prosperity depends peace. In saying , posing moralists , contrary, humility expressing conviction fellow Member States attached principles .The attitude Afghanistan delegation previous sessions Assembly inspired principles, will continue case current session. This attitude characterized kind opposition towards country group countries, idea different basic ideal United Nations , distinction among Member States based geographical considerations.This world scene tremendous evolution last twelve years. The nationalist movements African Asian continents natural European American continents last century. Thus, supported movements, basis objective appreciation fact sincere desire problems world settled mutual understanding goodwill, violent reactions bloodshed. We likewise follow line today, without antagonistic fanatical feelings towards country.The independence Federation Malaya admission United Nations constitute one best examples, justification rightful aspirations great nation also generous gesture appreciation great Power, United Kingdom. Both sincerely congratulated, examples followed similar cases.We realize United Nations difficulties present world problems considered many perspectives. We also realize human problems complicated, national political economic problems play role, individual problem regarded complete detachment currents world problems. Notwithstanding difficulties, however, reason appreciate normal evolution world.Fortunately, realize great Organization, zeal eminent Secretary-General devoted officials, gone forward along path traced twelve years ago. This cause hope evidence progress; believe devotion adherence principles spirit Charter shall gradually overcome many difficulties.Our optimism based consciousness peoples world , peace, alternative ultimately identify respective policies spirit Charter. Each every one us convinced another war bring nothing complete annihilation good, worth- beautiful life - perhaps life .We Afghans ambition preserve freedom try ensure prosperity people order may live modern nation world. We attached traditions spiritual legacy well Constitution, spontaneous outgrowth nature country. We trying preserve , ready, always , defend freedom integrity - words. Fortunately, stability position sincerity neutrality tested proved last fifth years, especially two world wars. To develop modernize country need support assistance developed countries, grateful receive aid.We greatly appreciate technical assistance received United Nations; appreciate value, well opportunity given us closer co-operation world Organization.Afghanistan believes peaceful settlement international differences problems, great small. We tried past, success, settle many problems direct negotiations, use good offices, advice technical help friends, peaceful means conciliation. We trying now, shall try future, settle problems means, basis objective, unprejudiced consideration principles right justice.In view, regards problems world conflicts arise, ultimate reference unsettled problems , ether conciliatory means settlement fail, United Nations International Court Justice. We many important problems agenda year will debated present session. The future world preservation international peace depend . We hope, like everyone , problems will find satisfactory just solution spirit United Nations Charter. To end prepared contribute modest way facilitating solution.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('../../data/processed/light.csv')\n",
    "# Filter\n",
    "timestamps = df.year.to_list()\n",
    "texts = df.text.to_list()\n",
    "\n",
    "text = texts[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e94e422f-b529-4f08-8e67-aa36d1726319",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'it', 'privilege', 'express', ',', 'mr', '.', 'president', ',', 'congratulations', 'afghanistan', 'delegation', 'election', ',', 'just', '##ly', 'unanimously', 'voted', 'assembly', '.', 'it', 'also', 'privilege', 'extend', 'fellow', 'representatives', 'greeting', '##s', 'royal', 'afghan', 'government', ',', 'well', 'sincere', '##st', 'wishes', 'success', 'current', 'session', 'general', 'assembly', '.', 'our', 'attachment', 'united', 'nations', 'charter', 'principles', 'complete', 'adherence', 'principles', 'human', 'rights', 'self', '-', 'determination', 'peoples', 'based', 'ideological', 'grounds', 'also', 'result', 'long', 'experience', 'free', 'small', 'country', 'controversial', 'events', 'modern', 'history', '.', 'we', 'believe', 'peace', 'world', 'can', 'secured', 'bases', ',', 'certain', 'future', 'prosperity', 'depends', 'peace', '.', 'in', 'saying', ',', 'posing', 'moral', '##ists', ',', 'contrary', ',', 'hum', '##ility', 'expressing', 'conviction', 'fellow', 'member', 'states', 'attached', 'principles', '.', 'the', 'attitude', 'afghanistan', 'delegation', 'previous', 'sessions', 'assembly', 'inspired', 'principles', ',', 'will', 'continue', 'case', 'current', 'session', '.', 'this', 'attitude', 'characterized', 'kind', 'opposition', 'towards', 'country', 'group', 'countries', ',', 'idea', 'different', 'basic', 'ideal', 'united', 'nations', ',', 'distinction', 'among', 'member', 'states', 'based', 'geographical', 'considerations', '.', 'this', 'world', 'scene', 'tremendous', 'evolution', 'last', 'twelve', 'years', '.', 'the', 'nationalist', 'movements', 'african', 'asian', 'continents', 'natural', 'european', 'american', 'continents', 'last', 'century', '.', 'thus', ',', 'supported', 'movements', ',', 'basis', 'objective', 'appreciation', 'fact', 'sincere', 'desire', 'problems', 'world', 'settled', 'mutual', 'understanding', 'goodwill', ',', 'violent', 'reactions', 'blood', '##shed', '.', 'we', 'likewise', 'follow', 'line', 'today', ',', 'without', 'antagonist', '##ic', 'fan', '##atic', '##al', 'feelings', 'towards', 'country', '.', 'the', 'independence', 'federation', 'malaya', 'admission', 'united', 'nations', 'constitute', 'one', 'best', 'examples', ',', 'justification', 'rightful', 'aspirations', 'great', 'nation', 'also', 'generous', 'gesture', 'appreciation', 'great', 'power', ',', 'united', 'kingdom', '.', 'both', 'sincerely', 'cong', '##rat', '##ulated', ',', 'examples', 'followed', 'similar', 'cases', '.', 'we', 'realize', 'united', 'nations', 'difficulties', 'present', 'world', 'problems', 'considered', 'many', 'perspectives', '.', 'we', 'also', 'realize', 'human', 'problems', 'complicated', ',', 'national', 'political', 'economic', 'problems', 'play', 'role', ',', 'individual', 'problem', 'regarded', 'complete', 'detachment', 'currents', 'world', 'problems', '.', 'notwithstanding', 'difficulties', ',', 'however', ',', 'reason', 'appreciate', 'normal', 'evolution', 'world', '.', 'fortunately', ',', 'realize', 'great', 'organization', ',', 'ze', '##al', 'eminent', 'secretary', '-', 'general', 'devoted', 'officials', ',', 'gone', 'forward', 'along', 'path', 'traced', 'twelve', 'years', 'ago', '.', 'this', 'cause', 'hope', 'evidence', 'progress', ';', 'believe', 'devotion', 'adherence', 'principles', 'spirit', 'charter', 'shall', 'gradually', 'overcome', 'many', 'difficulties', '.', 'our', 'optimism', 'based', 'consciousness', 'peoples', 'world', ',', 'peace', ',', 'alternative', 'ultimately', 'identify', 'respective', 'policies', 'spirit', 'charter', '.', 'each', 'every', 'one', 'us', 'convinced', 'another', 'war', 'bring', 'nothing', 'complete', 'ann', '##ih', '##ilation', 'good', ',', 'worth', '-', 'beautiful', 'life', '-', 'perhaps', 'life', '.', 'we', 'afghan', '##s', 'ambition', 'preserve', 'freedom', 'try', 'ensure', 'prosperity', 'people', 'order', 'may', 'live', 'modern', 'nation', 'world', '.', 'we', 'attached', 'traditions', 'spiritual', 'legacy', 'well', 'constitution', ',', 'spontaneous', 'out', '##growth', 'nature', 'country', '.', 'we', 'trying', 'preserve', ',', 'ready', ',', 'always', ',', 'defend', 'freedom', 'integrity', '-', 'words', '.', 'fortunately', ',', 'stability', 'position', 'sincerity', 'neutrality', 'tested', 'proved', 'last', 'fifth', 'years', ',', 'especially', 'two', 'world', 'wars', '.', 'to', 'develop', 'modern', '##ize', 'country', 'need', 'support', 'assistance', 'developed', 'countries', ',', 'grateful', 'receive', 'aid', '.', 'we', 'greatly', 'appreciate', 'technical', 'assistance', 'received', 'united', 'nations', ';', 'appreciate', 'value', ',', 'well', 'opportunity', 'given', 'us', 'closer', 'co', '-', 'operation', 'world', 'organization', '.', 'afghanistan', 'believes', 'peaceful', 'settlement', 'international', 'differences', 'problems', ',', 'great', 'small', '.', 'we', 'tried', 'past', ',', 'success', ',', 'settle', 'many', 'problems', 'direct', 'negotiations', ',', 'use', 'good', 'offices', ',', 'advice', 'technical', 'help', 'friends', ',', 'peaceful', 'means', 'con', '##ci', '##lia', '##tion', '.', 'we', 'trying', 'now', ',', 'shall', 'try', 'future', ',', 'settle', 'problems', 'means', ',', 'basis', 'objective', ',', 'un', '##pre', '##ju', '##dice', '##d', 'consideration', 'principles', 'right', 'justice', '.', 'in', 'view', ',', 'regards', 'problems', 'world', 'conflicts', 'arise', ',', 'ultimate', 'reference', 'un', '##sett', '##led', 'problems', ',', 'ether', 'con', '##ci', '##lia', '##tory', 'means', 'settlement', 'fail', ',', 'united', 'nations', 'international', 'court', 'justice', '.', 'we', 'many', 'important', 'problems', 'agenda', 'year', 'will', 'debated', 'present', 'session', '.', 'the', 'future', 'world', 'preservation', 'international', 'peace', 'depend', '.', 'we', 'hope', ',', 'like', 'everyone', ',', 'problems', 'will', 'find', 'satisfactory', 'just', 'solution', 'spirit', 'united', 'nations', 'charter', '.', 'to', 'end', 'prepared', 'contribute', 'modest', 'way', 'facilitating', 'solution', '.', '[SEP]']\n",
      "[CLS]           101\n",
      "it            2,009\n",
      "privilege    14,293\n",
      "express       4,671\n",
      ",             1,010\n",
      "mr            2,720\n",
      ".             1,012\n",
      "president     2,343\n",
      ",             1,010\n",
      "congratulations 23,156\n",
      "afghanistan   7,041\n",
      "delegation   10,656\n",
      "election      2,602\n",
      ",             1,010\n",
      "just          2,074\n",
      "##ly          2,135\n",
      "unanimously  15,645\n",
      "voted         5,444\n",
      "assembly      3,320\n",
      ".             1,012\n",
      "it            2,009\n",
      "also          2,036\n",
      "privilege    14,293\n",
      "extend        7,949\n",
      "fellow        3,507\n",
      "representatives  4,505\n",
      "greeting     14,806\n",
      "##s           2,015\n",
      "royal         2,548\n",
      "afghan       12,632\n",
      "government    2,231\n",
      ",             1,010\n",
      "well          2,092\n",
      "sincere      18,006\n",
      "##st          3,367\n",
      "wishes        8,996\n",
      "success       3,112\n",
      "current       2,783\n",
      "session       5,219\n",
      "general       2,236\n",
      "assembly      3,320\n",
      ".             1,012\n",
      "our           2,256\n",
      "attachment   14,449\n",
      "united        2,142\n",
      "nations       3,741\n",
      "charter       6,111\n",
      "principles    6,481\n",
      "complete      3,143\n",
      "adherence    29,235\n",
      "principles    6,481\n",
      "human         2,529\n",
      "rights        2,916\n",
      "self          2,969\n",
      "-             1,011\n",
      "determination  9,128\n",
      "peoples       7,243\n",
      "based         2,241\n",
      "ideological  17,859\n",
      "grounds       5,286\n",
      "also          2,036\n",
      "result        2,765\n",
      "long          2,146\n",
      "experience    3,325\n",
      "free          2,489\n",
      "small         2,235\n",
      "country       2,406\n",
      "controversial  6,801\n",
      "events        2,824\n",
      "modern        2,715\n",
      "history       2,381\n",
      ".             1,012\n",
      "we            2,057\n",
      "believe       2,903\n",
      "peace         3,521\n",
      "world         2,088\n",
      "can           2,064\n",
      "secured       7,119\n",
      "bases         7,888\n",
      ",             1,010\n",
      "certain       3,056\n",
      "future        2,925\n",
      "prosperity   14,165\n",
      "depends       9,041\n",
      "peace         3,521\n",
      ".             1,012\n",
      "in            1,999\n",
      "saying        3,038\n",
      ",             1,010\n",
      "posing       20,540\n",
      "moral         7,191\n",
      "##ists        5,130\n",
      ",             1,010\n",
      "contrary     10,043\n",
      ",             1,010\n",
      "hum          14,910\n",
      "##ility      15,148\n",
      "expressing   14,026\n",
      "conviction   10,652\n",
      "fellow        3,507\n",
      "member        2,266\n",
      "states        2,163\n",
      "attached      4,987\n",
      "principles    6,481\n",
      ".             1,012\n",
      "the           1,996\n",
      "attitude      7,729\n",
      "afghanistan   7,041\n",
      "delegation   10,656\n",
      "previous      3,025\n",
      "sessions      6,521\n",
      "assembly      3,320\n",
      "inspired      4,427\n",
      "principles    6,481\n",
      ",             1,010\n",
      "will          2,097\n",
      "continue      3,613\n",
      "case          2,553\n",
      "current       2,783\n",
      "session       5,219\n",
      ".             1,012\n",
      "this          2,023\n",
      "attitude      7,729\n",
      "characterized  7,356\n",
      "kind          2,785\n",
      "opposition    4,559\n",
      "towards       2,875\n",
      "country       2,406\n",
      "group         2,177\n",
      "countries     3,032\n",
      ",             1,010\n",
      "idea          2,801\n",
      "different     2,367\n",
      "basic         3,937\n",
      "ideal         7,812\n",
      "united        2,142\n",
      "nations       3,741\n",
      ",             1,010\n",
      "distinction   7,835\n",
      "among         2,426\n",
      "member        2,266\n",
      "states        2,163\n",
      "based         2,241\n",
      "geographical 10,056\n",
      "considerations 16,852\n",
      ".             1,012\n",
      "this          2,023\n",
      "world         2,088\n",
      "scene         3,496\n",
      "tremendous   14,388\n",
      "evolution     6,622\n",
      "last          2,197\n",
      "twelve        4,376\n",
      "years         2,086\n",
      ".             1,012\n",
      "the           1,996\n",
      "nationalist   8,986\n",
      "movements     5,750\n",
      "african       3,060\n",
      "asian         4,004\n",
      "continents   17,846\n",
      "natural       3,019\n",
      "european      2,647\n",
      "american      2,137\n",
      "continents   17,846\n",
      "last          2,197\n",
      "century       2,301\n",
      ".             1,012\n",
      "thus          2,947\n",
      ",             1,010\n",
      "supported     3,569\n",
      "movements     5,750\n",
      ",             1,010\n",
      "basis         3,978\n",
      "objective     7,863\n",
      "appreciation 12,284\n",
      "fact          2,755\n",
      "sincere      18,006\n",
      "desire        4,792\n",
      "problems      3,471\n",
      "world         2,088\n",
      "settled       3,876\n",
      "mutual        8,203\n",
      "understanding  4,824\n",
      "goodwill     22,875\n",
      ",             1,010\n",
      "violent       6,355\n",
      "reactions     9,597\n",
      "blood         2,668\n",
      "##shed       14,740\n",
      ".             1,012\n",
      "we            2,057\n",
      "likewise     10,655\n",
      "follow        3,582\n",
      "line          2,240\n",
      "today         2,651\n",
      ",             1,010\n",
      "without       2,302\n",
      "antagonist   17,379\n",
      "##ic          2,594\n",
      "fan           5,470\n",
      "##atic       12,070\n",
      "##al          2,389\n",
      "feelings      5,346\n",
      "towards       2,875\n",
      "country       2,406\n",
      ".             1,012\n",
      "the           1,996\n",
      "independence  4,336\n",
      "federation    4,657\n",
      "malaya       19,979\n",
      "admission     9,634\n",
      "united        2,142\n",
      "nations       3,741\n",
      "constitute   12,346\n",
      "one           2,028\n",
      "best          2,190\n",
      "examples      4,973\n",
      ",             1,010\n",
      "justification 19,777\n",
      "rightful     27,167\n",
      "aspirations  22,877\n",
      "great         2,307\n",
      "nation        3,842\n",
      "also          2,036\n",
      "generous     12,382\n",
      "gesture       9,218\n",
      "appreciation 12,284\n",
      "great         2,307\n",
      "power         2,373\n",
      ",             1,010\n",
      "united        2,142\n",
      "kingdom       2,983\n",
      ".             1,012\n",
      "both          2,119\n",
      "sincerely    25,664\n",
      "cong         26,478\n",
      "##rat         8,609\n",
      "##ulated      8,898\n",
      ",             1,010\n",
      "examples      4,973\n",
      "followed      2,628\n",
      "similar       2,714\n",
      "cases         3,572\n",
      ".             1,012\n",
      "we            2,057\n",
      "realize       5,382\n",
      "united        2,142\n",
      "nations       3,741\n",
      "difficulties  8,190\n",
      "present       2,556\n",
      "world         2,088\n",
      "problems      3,471\n",
      "considered    2,641\n",
      "many          2,116\n",
      "perspectives 15,251\n",
      ".             1,012\n",
      "we            2,057\n",
      "also          2,036\n",
      "realize       5,382\n",
      "human         2,529\n",
      "problems      3,471\n",
      "complicated   8,552\n",
      ",             1,010\n",
      "national      2,120\n",
      "political     2,576\n",
      "economic      3,171\n",
      "problems      3,471\n",
      "play          2,377\n",
      "role          2,535\n",
      ",             1,010\n",
      "individual    3,265\n",
      "problem       3,291\n",
      "regarded      5,240\n",
      "complete      3,143\n",
      "detachment   11,009\n",
      "currents     14,731\n",
      "world         2,088\n",
      "problems      3,471\n",
      ".             1,012\n",
      "notwithstanding 26,206\n",
      "difficulties  8,190\n",
      ",             1,010\n",
      "however       2,174\n",
      ",             1,010\n",
      "reason        3,114\n",
      "appreciate    9,120\n",
      "normal        3,671\n",
      "evolution     6,622\n",
      "world         2,088\n",
      ".             1,012\n",
      "fortunately  14,599\n",
      ",             1,010\n",
      "realize       5,382\n",
      "great         2,307\n",
      "organization  3,029\n",
      ",             1,010\n",
      "ze           27,838\n",
      "##al          2,389\n",
      "eminent      14,953\n",
      "secretary     3,187\n",
      "-             1,011\n",
      "general       2,236\n",
      "devoted       7,422\n",
      "officials     4,584\n",
      ",             1,010\n",
      "gone          2,908\n",
      "forward       2,830\n",
      "along         2,247\n",
      "path          4,130\n",
      "traced        9,551\n",
      "twelve        4,376\n",
      "years         2,086\n",
      "ago           3,283\n",
      ".             1,012\n",
      "this          2,023\n",
      "cause         3,426\n",
      "hope          3,246\n",
      "evidence      3,350\n",
      "progress      5,082\n",
      ";             1,025\n",
      "believe       2,903\n",
      "devotion     13,347\n",
      "adherence    29,235\n",
      "principles    6,481\n",
      "spirit        4,382\n",
      "charter       6,111\n",
      "shall         4,618\n",
      "gradually     6,360\n",
      "overcome      9,462\n",
      "many          2,116\n",
      "difficulties  8,190\n",
      ".             1,012\n",
      "our           2,256\n",
      "optimism     27,451\n",
      "based         2,241\n",
      "consciousness  8,298\n",
      "peoples       7,243\n",
      "world         2,088\n",
      ",             1,010\n",
      "peace         3,521\n",
      ",             1,010\n",
      "alternative   4,522\n",
      "ultimately    4,821\n",
      "identify      6,709\n",
      "respective    7,972\n",
      "policies      6,043\n",
      "spirit        4,382\n",
      "charter       6,111\n",
      ".             1,012\n",
      "each          2,169\n",
      "every         2,296\n",
      "one           2,028\n",
      "us            2,149\n",
      "convinced     6,427\n",
      "another       2,178\n",
      "war           2,162\n",
      "bring         3,288\n",
      "nothing       2,498\n",
      "complete      3,143\n",
      "ann           5,754\n",
      "##ih         19,190\n",
      "##ilation    29,545\n",
      "good          2,204\n",
      ",             1,010\n",
      "worth         4,276\n",
      "-             1,011\n",
      "beautiful     3,376\n",
      "life          2,166\n",
      "-             1,011\n",
      "perhaps       3,383\n",
      "life          2,166\n",
      ".             1,012\n",
      "we            2,057\n",
      "afghan       12,632\n",
      "##s           2,015\n",
      "ambition     16,290\n",
      "preserve      7,969\n",
      "freedom       4,071\n",
      "try           3,046\n",
      "ensure        5,676\n",
      "prosperity   14,165\n",
      "people        2,111\n",
      "order         2,344\n",
      "may           2,089\n",
      "live          2,444\n",
      "modern        2,715\n",
      "nation        3,842\n",
      "world         2,088\n",
      ".             1,012\n",
      "we            2,057\n",
      "attached      4,987\n",
      "traditions    7,443\n",
      "spiritual     6,259\n",
      "legacy        8,027\n",
      "well          2,092\n",
      "constitution  4,552\n",
      ",             1,010\n",
      "spontaneous  17,630\n",
      "out           2,041\n",
      "##growth     26,982\n",
      "nature        3,267\n",
      "country       2,406\n",
      ".             1,012\n",
      "we            2,057\n",
      "trying        2,667\n",
      "preserve      7,969\n",
      ",             1,010\n",
      "ready         3,201\n",
      ",             1,010\n",
      "always        2,467\n",
      ",             1,010\n",
      "defend        6,985\n",
      "freedom       4,071\n",
      "integrity    11,109\n",
      "-             1,011\n",
      "words         2,616\n",
      ".             1,012\n",
      "fortunately  14,599\n",
      ",             1,010\n",
      "stability     9,211\n",
      "position      2,597\n",
      "sincerity    23,997\n",
      "neutrality   21,083\n",
      "tested        7,718\n",
      "proved        4,928\n",
      "last          2,197\n",
      "fifth         3,587\n",
      "years         2,086\n",
      ",             1,010\n",
      "especially    2,926\n",
      "two           2,048\n",
      "world         2,088\n",
      "wars          5,233\n",
      ".             1,012\n",
      "to            2,000\n",
      "develop       4,503\n",
      "modern        2,715\n",
      "##ize         4,697\n",
      "country       2,406\n",
      "need          2,342\n",
      "support       2,490\n",
      "assistance    5,375\n",
      "developed     2,764\n",
      "countries     3,032\n",
      ",             1,010\n",
      "grateful      8,794\n",
      "receive       4,374\n",
      "aid           4,681\n",
      ".             1,012\n",
      "we            2,057\n",
      "greatly       6,551\n",
      "appreciate    9,120\n",
      "technical     4,087\n",
      "assistance    5,375\n",
      "received      2,363\n",
      "united        2,142\n",
      "nations       3,741\n",
      ";             1,025\n",
      "appreciate    9,120\n",
      "value         3,643\n",
      ",             1,010\n",
      "well          2,092\n",
      "opportunity   4,495\n",
      "given         2,445\n",
      "us            2,149\n",
      "closer        3,553\n",
      "co            2,522\n",
      "-             1,011\n",
      "operation     3,169\n",
      "world         2,088\n",
      "organization  3,029\n",
      ".             1,012\n",
      "afghanistan   7,041\n",
      "believes      7,164\n",
      "peaceful      9,379\n",
      "settlement    4,093\n",
      "international  2,248\n",
      "differences   5,966\n",
      "problems      3,471\n",
      ",             1,010\n",
      "great         2,307\n",
      "small         2,235\n",
      ".             1,012\n",
      "we            2,057\n",
      "tried         2,699\n",
      "past          2,627\n",
      ",             1,010\n",
      "success       3,112\n",
      ",             1,010\n",
      "settle        7,392\n",
      "many          2,116\n",
      "problems      3,471\n",
      "direct        3,622\n",
      "negotiations  7,776\n",
      ",             1,010\n",
      "use           2,224\n",
      "good          2,204\n",
      "offices       4,822\n",
      ",             1,010\n",
      "advice        6,040\n",
      "technical     4,087\n",
      "help          2,393\n",
      "friends       2,814\n",
      ",             1,010\n",
      "peaceful      9,379\n",
      "means         2,965\n",
      "con           9,530\n",
      "##ci          6,895\n",
      "##lia         6,632\n",
      "##tion        3,508\n",
      ".             1,012\n",
      "we            2,057\n",
      "trying        2,667\n",
      "now           2,085\n",
      ",             1,010\n",
      "shall         4,618\n",
      "try           3,046\n",
      "future        2,925\n",
      ",             1,010\n",
      "settle        7,392\n",
      "problems      3,471\n",
      "means         2,965\n",
      ",             1,010\n",
      "basis         3,978\n",
      "objective     7,863\n",
      ",             1,010\n",
      "un            4,895\n",
      "##pre        28,139\n",
      "##ju          9,103\n",
      "##dice       24,598\n",
      "##d           2,094\n",
      "consideration  9,584\n",
      "principles    6,481\n",
      "right         2,157\n",
      "justice       3,425\n",
      ".             1,012\n",
      "in            1,999\n",
      "view          3,193\n",
      ",             1,010\n",
      "regards      12,362\n",
      "problems      3,471\n",
      "world         2,088\n",
      "conflicts     9,755\n",
      "arise        13,368\n",
      ",             1,010\n",
      "ultimate      7,209\n",
      "reference     4,431\n",
      "un            4,895\n",
      "##sett       21,678\n",
      "##led         3,709\n",
      "problems      3,471\n",
      ",             1,010\n",
      "ether        28,855\n",
      "con           9,530\n",
      "##ci          6,895\n",
      "##lia         6,632\n",
      "##tory        7,062\n",
      "means         2,965\n",
      "settlement    4,093\n",
      "fail          8,246\n",
      ",             1,010\n",
      "united        2,142\n",
      "nations       3,741\n",
      "international  2,248\n",
      "court         2,457\n",
      "justice       3,425\n",
      ".             1,012\n",
      "we            2,057\n",
      "many          2,116\n",
      "important     2,590\n",
      "problems      3,471\n",
      "agenda       11,376\n",
      "year          2,095\n",
      "will          2,097\n",
      "debated      15,268\n",
      "present       2,556\n",
      "session       5,219\n",
      ".             1,012\n",
      "the           1,996\n",
      "future        2,925\n",
      "world         2,088\n",
      "preservation  8,347\n",
      "international  2,248\n",
      "peace         3,521\n",
      "depend       12,530\n",
      ".             1,012\n",
      "we            2,057\n",
      "hope          3,246\n",
      ",             1,010\n",
      "like          2,066\n",
      "everyone      3,071\n",
      ",             1,010\n",
      "problems      3,471\n",
      "will          2,097\n",
      "find          2,424\n",
      "satisfactory 23,045\n",
      "just          2,074\n",
      "solution      5,576\n",
      "spirit        4,382\n",
      "united        2,142\n",
      "nations       3,741\n",
      "charter       6,111\n",
      ".             1,012\n",
      "to            2,000\n",
      "end           2,203\n",
      "prepared      4,810\n",
      "contribute    9,002\n",
      "modest       10,754\n",
      "way           2,126\n",
      "facilitating 25,505\n",
      "solution      5,576\n",
      ".             1,012\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'. format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "532f0d1f-5dff-4ac6-ada5-c2cd6ecbf47a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      False\n",
      "1      False\n",
      "2      False\n",
      "3      False\n",
      "4      False\n",
      "       ...  \n",
      "609    False\n",
      "610    False\n",
      "611    False\n",
      "612    False\n",
      "613    False\n",
      "Length: 614, dtype: bool\n",
      "0      NaN\n",
      "1      NaN\n",
      "2      NaN\n",
      "3      NaN\n",
      "4      NaN\n",
      "      ... \n",
      "609    NaN\n",
      "610    NaN\n",
      "611    NaN\n",
      "612    NaN\n",
      "613    NaN\n",
      "Length: 614, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "614"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_words=pd.Series(tokenized_text)\n",
    "print(pd_words==\"the\")\n",
    "print(pd_words.where(pd_words==\"the\"))\n",
    "\n",
    "list_index = list(pd_words.index[pd_words==\"the\"])\n",
    "\n",
    "\n",
    "len(list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "467b9450-a411-450c-b8d1-b28b0296e849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "segments_ids = [1] * len(tokenized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "12e4141e-ca76-4762-92d0-58f00c3fb058",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "tokens_tensor = tokens_tensor[:,0:512]\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "segments_tensors = segments_tensors[:,0:512]\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased', \n",
    "                                  output_hidden_states = True)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "    hidden_states = outputs[2]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0f86fef3-bdf4-45fe-bf1c-363ab05defa7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "315a0603-1b3d-4cf1-b7bf-98defbd78fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
      "Number of batches: 1\n",
      "Number of tokens: 512\n",
      "      Type of hidden_states:  <class 'tuple'>\n",
      "Tensor shape for each layer:  torch.Size([1, 512, 768])\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print('      Type of hidden_states: ', type(hidden_states))\n",
    "\n",
    "# Each layer in the \n",
    "t is a torch tensor.\n",
    "print('Tensor shape for each layer: ', hidden_states[0].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6ce978e1-9f42-4017-832e-317212abd66d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 512, 768])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "token_embeddings.size()\n",
    "\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "token_embeddings.size()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6d4136dc-ccb0-446d-b593-36650757cdb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 13, 768])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embeddings.permute(1, 0, 2)\n",
    "print(token_embeddings.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3d94540e-a777-469b-9f1d-b3013b14502c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_embeddings_subset = token_embeddings[:,-1, :].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "12a576cc-0ae3-4557-bb9a-a404b18899c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings_subset[list_words.index(\"the\"), :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "8786f236-5c9d-4831-95ae-1d74849af763",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ']' (239714737.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[208], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    token_embeddings_subset[], :]\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ']'\n"
     ]
    }
   ],
   "source": [
    "list_index.pop()\n",
    "\n",
    "token_embeddings_subset[list_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "05c35ba5-d2fe-4d2e-8da8-6e5434544450",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.6855e-01, -2.8577e-01, -3.2613e-01,  ..., -2.7571e-02,\n",
      "           3.8253e-02,  1.6400e-01],\n",
      "         [ 3.5559e-02, -4.6038e-03, -1.6089e-01,  ...,  2.1338e-01,\n",
      "          -7.7714e-02,  7.0369e-02],\n",
      "         [ 5.8702e-02, -1.8409e-01, -2.6115e-01,  ...,  4.8097e-01,\n",
      "          -1.8088e-02, -3.7339e-02],\n",
      "         ...,\n",
      "         [ 1.5614e-02, -8.0550e-01,  1.2749e+00,  ..., -1.4153e+00,\n",
      "           3.9181e-01, -8.6010e-01],\n",
      "         [-1.3222e-01, -8.0118e-01,  7.2541e-01,  ..., -1.2620e+00,\n",
      "           1.2361e-01, -8.2128e-01],\n",
      "         [ 1.5054e-01, -2.5000e-01,  3.7395e-01,  ..., -5.4205e-01,\n",
      "           8.6377e-02, -5.3121e-01]],\n",
      "\n",
      "        [[-4.2272e-01, -2.8894e-02, -1.4556e-01,  ...,  7.1141e-01,\n",
      "           9.9297e-01,  1.9780e-01],\n",
      "         [-4.3760e-01, -5.4480e-01, -9.2192e-04,  ...,  5.0488e-01,\n",
      "           7.6371e-01,  6.4694e-01],\n",
      "         [-4.9401e-01, -6.1803e-01, -1.4912e-01,  ...,  7.5252e-01,\n",
      "           5.2793e-01,  6.6038e-01],\n",
      "         ...,\n",
      "         [-9.2898e-01,  7.9915e-01,  1.7297e+00,  ..., -1.1976e+00,\n",
      "           8.1879e-01, -4.4115e-01],\n",
      "         [-1.4229e+00,  5.8391e-01,  1.4728e+00,  ..., -1.0999e+00,\n",
      "           4.5515e-01, -5.3198e-01],\n",
      "         [-8.5127e-01,  1.3517e-01,  7.0376e-01,  ..., -7.7584e-01,\n",
      "           4.4334e-02, -2.6722e-01]],\n",
      "\n",
      "        [[-6.5103e-02, -4.5141e-01, -1.0790e+00,  ..., -6.1097e-01,\n",
      "           5.8962e-01, -6.7774e-02],\n",
      "         [ 1.3912e-01, -4.9808e-01, -8.1500e-01,  ..., -5.0809e-01,\n",
      "           9.8175e-01,  3.3649e-01],\n",
      "         [ 2.1573e-01, -7.0648e-01, -6.7653e-01,  ..., -5.2627e-02,\n",
      "           1.4800e+00,  4.9578e-01],\n",
      "         ...,\n",
      "         [-3.1495e-01,  1.2566e+00,  2.4075e+00,  ..., -1.2068e+00,\n",
      "           8.7750e-01, -3.2080e-01],\n",
      "         [-8.4763e-01,  9.7022e-01,  1.9502e+00,  ..., -1.2458e+00,\n",
      "           5.4987e-01, -5.2279e-01],\n",
      "         [-5.4699e-01,  4.3945e-01,  1.0700e+00,  ..., -7.5293e-01,\n",
      "           1.3558e-01, -2.7355e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.3157e-01, -1.2241e+00, -4.7952e-01,  ...,  4.7021e-02,\n",
      "           1.8299e-01, -1.8346e-01],\n",
      "         [ 4.3573e-02, -1.1971e+00, -5.5169e-01,  ..., -1.6400e-01,\n",
      "           5.5696e-01,  3.7322e-01],\n",
      "         [-4.3007e-01, -7.5259e-01, -1.9815e-02,  ..., -6.3494e-01,\n",
      "           7.4057e-01,  3.7624e-02],\n",
      "         ...,\n",
      "         [-3.7988e-01, -1.9162e-01,  8.2641e-01,  ..., -1.2081e+00,\n",
      "           5.6962e-01, -1.1940e-01],\n",
      "         [-1.1139e+00, -5.2637e-01,  5.0433e-01,  ..., -1.2998e+00,\n",
      "          -9.4325e-02, -5.8710e-02],\n",
      "         [-6.3929e-01, -3.0820e-01,  1.7162e-01,  ..., -3.4353e-01,\n",
      "          -1.4369e-01, -2.6760e-01]],\n",
      "\n",
      "        [[-2.7445e-02, -2.8260e-01,  1.3628e-01,  ..., -7.7692e-01,\n",
      "           8.3417e-01, -3.0185e-01],\n",
      "         [-4.6664e-01, -2.5957e-01, -2.5880e-01,  ..., -8.9368e-01,\n",
      "           1.1677e+00,  3.2202e-01],\n",
      "         [-7.4007e-01, -2.7520e-01,  3.3366e-01,  ..., -6.6552e-01,\n",
      "           1.9216e+00,  1.7115e-01],\n",
      "         ...,\n",
      "         [-6.5088e-01, -1.6004e-01,  6.0248e-01,  ..., -1.1080e+00,\n",
      "           6.5079e-01, -5.6596e-01],\n",
      "         [-1.2983e+00, -7.5035e-01,  2.7386e-01,  ..., -1.1788e+00,\n",
      "          -3.9430e-02, -3.8126e-01],\n",
      "         [-5.2812e-01, -2.7760e-01,  6.5384e-02,  ..., -2.6761e-01,\n",
      "          -2.2952e-01, -3.9690e-01]],\n",
      "\n",
      "        [[-1.5077e-01, -8.0116e-02,  1.6308e-01,  ...,  5.0548e-01,\n",
      "           9.6337e-01, -8.8385e-01],\n",
      "         [-3.6368e-01, -1.7990e-01,  1.9006e-01,  ...,  4.6302e-01,\n",
      "           6.2044e-01, -6.5172e-01],\n",
      "         [-3.9146e-01,  2.5755e-01,  5.2306e-01,  ...,  7.6026e-01,\n",
      "           5.8749e-01, -5.7371e-01],\n",
      "         ...,\n",
      "         [-5.9349e-01,  6.5778e-01,  1.1988e+00,  ..., -1.2970e+00,\n",
      "           6.7153e-01, -1.4651e+00],\n",
      "         [-1.1883e+00, -7.7792e-02,  1.0729e+00,  ..., -9.7063e-01,\n",
      "           2.8388e-01, -1.4442e+00],\n",
      "         [-5.3273e-01, -4.5453e-02,  4.2272e-01,  ..., -2.0815e-01,\n",
      "           8.9591e-02, -8.5020e-01]]])\n"
     ]
    }
   ],
   "source": [
    "print(token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5538f1f5-5893-47b0-80c0-0b0dabf9e15c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 512 x 3072\n"
     ]
    }
   ],
   "source": [
    "token_vecs_cat = []\n",
    "for token in token_embeddings:\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    token_vecs_cat.append(cat_vec)\n",
    "    \n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9311c6ab-b760-430b-acfe-d90d44acb0fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 512 x 768\n",
      "0 [CLS]\n",
      "1 it\n",
      "2 privilege\n",
      "3 express\n",
      "4 ,\n",
      "5 mr\n",
      "6 .\n",
      "7 president\n",
      "8 ,\n",
      "9 congratulations\n",
      "10 afghanistan\n",
      "11 delegation\n",
      "12 election\n",
      "13 ,\n",
      "14 just\n",
      "15 ##ly\n",
      "16 unanimously\n",
      "17 voted\n",
      "18 assembly\n",
      "19 .\n",
      "20 it\n",
      "21 also\n",
      "22 privilege\n",
      "23 extend\n",
      "24 fellow\n",
      "25 representatives\n",
      "26 greeting\n",
      "27 ##s\n",
      "28 royal\n",
      "29 afghan\n",
      "30 government\n",
      "31 ,\n",
      "32 well\n",
      "33 sincere\n",
      "34 ##st\n",
      "35 wishes\n",
      "36 success\n",
      "37 current\n",
      "38 session\n",
      "39 general\n",
      "40 assembly\n",
      "41 .\n",
      "42 our\n",
      "43 attachment\n",
      "44 united\n",
      "45 nations\n",
      "46 charter\n",
      "47 principles\n",
      "48 complete\n",
      "49 adherence\n",
      "50 principles\n",
      "51 human\n",
      "52 rights\n",
      "53 self\n",
      "54 -\n",
      "55 determination\n",
      "56 peoples\n",
      "57 based\n",
      "58 ideological\n",
      "59 grounds\n",
      "60 also\n",
      "61 result\n",
      "62 long\n",
      "63 experience\n",
      "64 free\n",
      "65 small\n",
      "66 country\n",
      "67 controversial\n",
      "68 events\n",
      "69 modern\n",
      "70 history\n",
      "71 .\n",
      "72 we\n",
      "73 believe\n",
      "74 peace\n",
      "75 world\n",
      "76 can\n",
      "77 secured\n",
      "78 bases\n",
      "79 ,\n",
      "80 certain\n",
      "81 future\n",
      "82 prosperity\n",
      "83 depends\n",
      "84 peace\n",
      "85 .\n",
      "86 in\n",
      "87 saying\n",
      "88 ,\n",
      "89 posing\n",
      "90 moral\n",
      "91 ##ists\n",
      "92 ,\n",
      "93 contrary\n",
      "94 ,\n",
      "95 hum\n",
      "96 ##ility\n",
      "97 expressing\n",
      "98 conviction\n",
      "99 fellow\n",
      "100 member\n",
      "101 states\n",
      "102 attached\n",
      "103 principles\n",
      "104 .\n",
      "105 the\n",
      "106 attitude\n",
      "107 afghanistan\n",
      "108 delegation\n",
      "109 previous\n",
      "110 sessions\n",
      "111 assembly\n",
      "112 inspired\n",
      "113 principles\n",
      "114 ,\n",
      "115 will\n",
      "116 continue\n",
      "117 case\n",
      "118 current\n",
      "119 session\n",
      "120 .\n",
      "121 this\n",
      "122 attitude\n",
      "123 characterized\n",
      "124 kind\n",
      "125 opposition\n",
      "126 towards\n",
      "127 country\n",
      "128 group\n",
      "129 countries\n",
      "130 ,\n",
      "131 idea\n",
      "132 different\n",
      "133 basic\n",
      "134 ideal\n",
      "135 united\n",
      "136 nations\n",
      "137 ,\n",
      "138 distinction\n",
      "139 among\n",
      "140 member\n",
      "141 states\n",
      "142 based\n",
      "143 geographical\n",
      "144 considerations\n",
      "145 .\n",
      "146 this\n",
      "147 world\n",
      "148 scene\n",
      "149 tremendous\n",
      "150 evolution\n",
      "151 last\n",
      "152 twelve\n",
      "153 years\n",
      "154 .\n",
      "155 the\n",
      "156 nationalist\n",
      "157 movements\n",
      "158 african\n",
      "159 asian\n",
      "160 continents\n",
      "161 natural\n",
      "162 european\n",
      "163 american\n",
      "164 continents\n",
      "165 last\n",
      "166 century\n",
      "167 .\n",
      "168 thus\n",
      "169 ,\n",
      "170 supported\n",
      "171 movements\n",
      "172 ,\n",
      "173 basis\n",
      "174 objective\n",
      "175 appreciation\n",
      "176 fact\n",
      "177 sincere\n",
      "178 desire\n",
      "179 problems\n",
      "180 world\n",
      "181 settled\n",
      "182 mutual\n",
      "183 understanding\n",
      "184 goodwill\n",
      "185 ,\n",
      "186 violent\n",
      "187 reactions\n",
      "188 blood\n",
      "189 ##shed\n",
      "190 .\n",
      "191 we\n",
      "192 likewise\n",
      "193 follow\n",
      "194 line\n",
      "195 today\n",
      "196 ,\n",
      "197 without\n",
      "198 antagonist\n",
      "199 ##ic\n",
      "200 fan\n",
      "201 ##atic\n",
      "202 ##al\n",
      "203 feelings\n",
      "204 towards\n",
      "205 country\n",
      "206 .\n",
      "207 the\n",
      "208 independence\n",
      "209 federation\n",
      "210 malaya\n",
      "211 admission\n",
      "212 united\n",
      "213 nations\n",
      "214 constitute\n",
      "215 one\n",
      "216 best\n",
      "217 examples\n",
      "218 ,\n",
      "219 justification\n",
      "220 rightful\n",
      "221 aspirations\n",
      "222 great\n",
      "223 nation\n",
      "224 also\n",
      "225 generous\n",
      "226 gesture\n",
      "227 appreciation\n",
      "228 great\n",
      "229 power\n",
      "230 ,\n",
      "231 united\n",
      "232 kingdom\n",
      "233 .\n",
      "234 both\n",
      "235 sincerely\n",
      "236 cong\n",
      "237 ##rat\n",
      "238 ##ulated\n",
      "239 ,\n",
      "240 examples\n",
      "241 followed\n",
      "242 similar\n",
      "243 cases\n",
      "244 .\n",
      "245 we\n",
      "246 realize\n",
      "247 united\n",
      "248 nations\n",
      "249 difficulties\n",
      "250 present\n",
      "251 world\n",
      "252 problems\n",
      "253 considered\n",
      "254 many\n",
      "255 perspectives\n",
      "256 .\n",
      "257 we\n",
      "258 also\n",
      "259 realize\n",
      "260 human\n",
      "261 problems\n",
      "262 complicated\n",
      "263 ,\n",
      "264 national\n",
      "265 political\n",
      "266 economic\n",
      "267 problems\n",
      "268 play\n",
      "269 role\n",
      "270 ,\n",
      "271 individual\n",
      "272 problem\n",
      "273 regarded\n",
      "274 complete\n",
      "275 detachment\n",
      "276 currents\n",
      "277 world\n",
      "278 problems\n",
      "279 .\n",
      "280 notwithstanding\n",
      "281 difficulties\n",
      "282 ,\n",
      "283 however\n",
      "284 ,\n",
      "285 reason\n",
      "286 appreciate\n",
      "287 normal\n",
      "288 evolution\n",
      "289 world\n",
      "290 .\n",
      "291 fortunately\n",
      "292 ,\n",
      "293 realize\n",
      "294 great\n",
      "295 organization\n",
      "296 ,\n",
      "297 ze\n",
      "298 ##al\n",
      "299 eminent\n",
      "300 secretary\n",
      "301 -\n",
      "302 general\n",
      "303 devoted\n",
      "304 officials\n",
      "305 ,\n",
      "306 gone\n",
      "307 forward\n",
      "308 along\n",
      "309 path\n",
      "310 traced\n",
      "311 twelve\n",
      "312 years\n",
      "313 ago\n",
      "314 .\n",
      "315 this\n",
      "316 cause\n",
      "317 hope\n",
      "318 evidence\n",
      "319 progress\n",
      "320 ;\n",
      "321 believe\n",
      "322 devotion\n",
      "323 adherence\n",
      "324 principles\n",
      "325 spirit\n",
      "326 charter\n",
      "327 shall\n",
      "328 gradually\n",
      "329 overcome\n",
      "330 many\n",
      "331 difficulties\n",
      "332 .\n",
      "333 our\n",
      "334 optimism\n",
      "335 based\n",
      "336 consciousness\n",
      "337 peoples\n",
      "338 world\n",
      "339 ,\n",
      "340 peace\n",
      "341 ,\n",
      "342 alternative\n",
      "343 ultimately\n",
      "344 identify\n",
      "345 respective\n",
      "346 policies\n",
      "347 spirit\n",
      "348 charter\n",
      "349 .\n",
      "350 each\n",
      "351 every\n",
      "352 one\n",
      "353 us\n",
      "354 convinced\n",
      "355 another\n",
      "356 war\n",
      "357 bring\n",
      "358 nothing\n",
      "359 complete\n",
      "360 ann\n",
      "361 ##ih\n",
      "362 ##ilation\n",
      "363 good\n",
      "364 ,\n",
      "365 worth\n",
      "366 -\n",
      "367 beautiful\n",
      "368 life\n",
      "369 -\n",
      "370 perhaps\n",
      "371 life\n",
      "372 .\n",
      "373 we\n",
      "374 afghan\n",
      "375 ##s\n",
      "376 ambition\n",
      "377 preserve\n",
      "378 freedom\n",
      "379 try\n",
      "380 ensure\n",
      "381 prosperity\n",
      "382 people\n",
      "383 order\n",
      "384 may\n",
      "385 live\n",
      "386 modern\n",
      "387 nation\n",
      "388 world\n",
      "389 .\n",
      "390 we\n",
      "391 attached\n",
      "392 traditions\n",
      "393 spiritual\n",
      "394 legacy\n",
      "395 well\n",
      "396 constitution\n",
      "397 ,\n",
      "398 spontaneous\n",
      "399 out\n",
      "400 ##growth\n",
      "401 nature\n",
      "402 country\n",
      "403 .\n",
      "404 we\n",
      "405 trying\n",
      "406 preserve\n",
      "407 ,\n",
      "408 ready\n",
      "409 ,\n",
      "410 always\n",
      "411 ,\n",
      "412 defend\n",
      "413 freedom\n",
      "414 integrity\n",
      "415 -\n",
      "416 words\n",
      "417 .\n",
      "418 fortunately\n",
      "419 ,\n",
      "420 stability\n",
      "421 position\n",
      "422 sincerity\n",
      "423 neutrality\n",
      "424 tested\n",
      "425 proved\n",
      "426 last\n",
      "427 fifth\n",
      "428 years\n",
      "429 ,\n",
      "430 especially\n",
      "431 two\n",
      "432 world\n",
      "433 wars\n",
      "434 .\n",
      "435 to\n",
      "436 develop\n",
      "437 modern\n",
      "438 ##ize\n",
      "439 country\n",
      "440 need\n",
      "441 support\n",
      "442 assistance\n",
      "443 developed\n",
      "444 countries\n",
      "445 ,\n",
      "446 grateful\n",
      "447 receive\n",
      "448 aid\n",
      "449 .\n",
      "450 we\n",
      "451 greatly\n",
      "452 appreciate\n",
      "453 technical\n",
      "454 assistance\n",
      "455 received\n",
      "456 united\n",
      "457 nations\n",
      "458 ;\n",
      "459 appreciate\n",
      "460 value\n",
      "461 ,\n",
      "462 well\n",
      "463 opportunity\n",
      "464 given\n",
      "465 us\n",
      "466 closer\n",
      "467 co\n",
      "468 -\n",
      "469 operation\n",
      "470 world\n",
      "471 organization\n",
      "472 .\n",
      "473 afghanistan\n",
      "474 believes\n",
      "475 peaceful\n",
      "476 settlement\n",
      "477 international\n",
      "478 differences\n",
      "479 problems\n",
      "480 ,\n",
      "481 great\n",
      "482 small\n",
      "483 .\n",
      "484 we\n",
      "485 tried\n",
      "486 past\n",
      "487 ,\n",
      "488 success\n",
      "489 ,\n",
      "490 settle\n",
      "491 many\n",
      "492 problems\n",
      "493 direct\n",
      "494 negotiations\n",
      "495 ,\n",
      "496 use\n",
      "497 good\n",
      "498 offices\n",
      "499 ,\n",
      "500 advice\n",
      "501 technical\n",
      "502 help\n",
      "503 friends\n",
      "504 ,\n",
      "505 peaceful\n",
      "506 means\n",
      "507 con\n",
      "508 ##ci\n",
      "509 ##lia\n",
      "510 ##tion\n",
      "511 .\n",
      "512 we\n",
      "513 trying\n",
      "514 now\n",
      "515 ,\n",
      "516 shall\n",
      "517 try\n",
      "518 future\n",
      "519 ,\n",
      "520 settle\n",
      "521 problems\n",
      "522 means\n",
      "523 ,\n",
      "524 basis\n",
      "525 objective\n",
      "526 ,\n",
      "527 un\n",
      "528 ##pre\n",
      "529 ##ju\n",
      "530 ##dice\n",
      "531 ##d\n",
      "532 consideration\n",
      "533 principles\n",
      "534 right\n",
      "535 justice\n",
      "536 .\n",
      "537 in\n",
      "538 view\n",
      "539 ,\n",
      "540 regards\n",
      "541 problems\n",
      "542 world\n",
      "543 conflicts\n",
      "544 arise\n",
      "545 ,\n",
      "546 ultimate\n",
      "547 reference\n",
      "548 un\n",
      "549 ##sett\n",
      "550 ##led\n",
      "551 problems\n",
      "552 ,\n",
      "553 ether\n",
      "554 con\n",
      "555 ##ci\n",
      "556 ##lia\n",
      "557 ##tory\n",
      "558 means\n",
      "559 settlement\n",
      "560 fail\n",
      "561 ,\n",
      "562 united\n",
      "563 nations\n",
      "564 international\n",
      "565 court\n",
      "566 justice\n",
      "567 .\n",
      "568 we\n",
      "569 many\n",
      "570 important\n",
      "571 problems\n",
      "572 agenda\n",
      "573 year\n",
      "574 will\n",
      "575 debated\n",
      "576 present\n",
      "577 session\n",
      "578 .\n",
      "579 the\n",
      "580 future\n",
      "581 world\n",
      "582 preservation\n",
      "583 international\n",
      "584 peace\n",
      "585 depend\n",
      "586 .\n",
      "587 we\n",
      "588 hope\n",
      "589 ,\n",
      "590 like\n",
      "591 everyone\n",
      "592 ,\n",
      "593 problems\n",
      "594 will\n",
      "595 find\n",
      "596 satisfactory\n",
      "597 just\n",
      "598 solution\n",
      "599 spirit\n",
      "600 united\n",
      "601 nations\n",
      "602 charter\n",
      "603 .\n",
      "604 to\n",
      "605 end\n",
      "606 prepared\n",
      "607 contribute\n",
      "608 modest\n",
      "609 way\n",
      "610 facilitating\n",
      "611 solution\n",
      "612 .\n",
      "613 [SEP]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "token_vecs_sum = []\n",
    "\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))\n",
    "token_vecs = hidden_states[-2][0]\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "\n",
    "for i, token_str in enumerate(tokenized_text):\n",
    "    print(i, token_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "9e710105-3a3c-46b6-973c-cc8a2696252a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "First 5 vector values for each instance of \"keywords\".\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[202], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFirst 5 vector values for each instance of \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst the   \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(list_index[\u001b[38;5;241m1\u001b[39m][:\u001b[38;5;241m5\u001b[39m]))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msecond the  \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(list_index[\u001b[38;5;241m2\u001b[39m][:\u001b[38;5;241m5\u001b[39m]))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "print(len(token_vecs_sum))\n",
    "list_index\n",
    "\n",
    "print('First 5 vector values for each instance of \"keywords\".')\n",
    "print('')\n",
    "print(\"first the   \", str(list_index[1][:5]))\n",
    "print(\"second the  \", str(list_index[2][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "cc20d292-ef8b-42eb-b772-4dcf3a579cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_vecs_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "552059f9-f8d0-46c7-a392-5480827d67e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bert_text_preparation(text, tokenizer):\n",
    "    \"\"\"Preparing the input for BERT\n",
    "    \n",
    "    Takes a string argument and performs\n",
    "    pre-processing like adding special tokens,\n",
    "    tokenization, tokens to ids, and tokens to\n",
    "    segment ids. All tokens are mapped to seg-\n",
    "    ment id = 1.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to be converted\n",
    "        tokenizer (obj): Tokenizer object\n",
    "            to convert text into BERT-re-\n",
    "            adable tokens and ids\n",
    "        \n",
    "    Returns:\n",
    "        list: List of BERT-readable tokens\n",
    "        obj: Torch tensor with token ids\n",
    "        obj: Torch tensor segment ids\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    return tokenized_text, tokens_tensor, segments_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bed305eb-7f42-413d-a021-d24f032d4c22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bert_embeddings(tokens_tensor, segments_tensors, model):\n",
    "    \"\"\"Get embeddings from an embedding model\n",
    "    \n",
    "    Args:\n",
    "        tokens_tensor (obj): Torch tensor size [n_tokens]\n",
    "            with token ids for each token in text\n",
    "        segments_tensors (obj): Torch tensor size [n_tokens]\n",
    "            with segment ids for each token in text\n",
    "        model (obj): Embedding model to generate embeddings\n",
    "            from token and segment ids\n",
    "    \n",
    "    Returns:\n",
    "        list: List of list of floats of size\n",
    "            [n_tokens, n_embedding_dimensions]\n",
    "            containing embeddings for each token\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Gradient calculation id disabled\n",
    "    # Model is in inference mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        # Removing the first hidden state\n",
    "        # The first state is the input state\n",
    "        #print(outputs[1:])\n",
    "        #original code was \n",
    "        hidden_states = outputs[2]\n",
    "        print(len(outputs))\n",
    " \n",
    "        \n",
    "    # Getting embeddings from the final BERT layer\n",
    "    token_embeddings = hidden_states[-1]\n",
    "    # Collapsing the tensor into 1-dimension\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=0)\n",
    "    token_embeddings.size()\n",
    "    # Converting torchtensors to lists\n",
    "    list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
    "    \n",
    "    \n",
    "    return list_token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e30f5588-0c17-482b-bf4d-b0b41b503998",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (614) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [1, 614].  Tensor sizes: [1, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts: \n\u001b[1;32m      4\u001b[0m     tokenized_text, tokens_tensor, segments_tensors \u001b[38;5;241m=\u001b[39m bert_text_preparation(text, tokenizer)\n\u001b[0;32m----> 5\u001b[0m     list_token_embeddings \u001b[38;5;241m=\u001b[39m get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
      "Cell \u001b[0;32mIn[76], line 22\u001b[0m, in \u001b[0;36mget_bert_embeddings\u001b[0;34m(tokens_tensor, segments_tensors, model)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Gradient calculation id disabled\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Model is in inference mode\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 22\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(tokens_tensor, segments_tensors)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Removing the first hidden state\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# The first state is the input state\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m#print(outputs[1:])\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m#original code was \u001b[39;00m\n\u001b[1;32m     27\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m2\u001b[39m][\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:988\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    987\u001b[0m     buffered_token_type_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mtoken_type_ids[:, :seq_length]\n\u001b[0;32m--> 988\u001b[0m     buffered_token_type_ids_expanded \u001b[38;5;241m=\u001b[39m buffered_token_type_ids\u001b[38;5;241m.\u001b[39mexpand(batch_size, seq_length)\n\u001b[1;32m    989\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m buffered_token_type_ids_expanded\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (614) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [1, 614].  Tensor sizes: [1, 512]"
     ]
    }
   ],
   "source": [
    "\n",
    "for text in texts: \n",
    "    tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(text, tokenizer)\n",
    "    list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a9e6d5a4-84ee-4de9-8aab-9c7b323b44c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'token_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tokens_vecs_cat \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m token_embeddings:\n\u001b[1;32m      3\u001b[0m     cat_vec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((token[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], token[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], token[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m], token[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m     tokens_vecs_cat\u001b[38;5;241m.\u001b[39mappend(cat_vec)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'token_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "tokens_vecs_cat = []\n",
    "for token in list_token_embeddings:\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    tokens_vecs_cat.append(cat_vec)\n",
    "print( 'Shape is: %d x %d' % (len(tokens_vecs_cat), len(tokens_vecs_cat[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8d6d0a2a-6162-400b-be90-3af65805eee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Calculating the distance between the\n",
    "# embeddings of 'bank' in all the\n",
    "# given contexts of the word\n",
    "\n",
    "list_of_distances = []\n",
    "for text1, embed1 in zip(texts, target_word_embeddings):\n",
    "    for text2, embed2 in zip(texts, target_word_embeddings):\n",
    "        cos_dist = 1 - cosine(embed1, embed2)\n",
    "        list_of_distances.append([text1, text2, cos_dist])\n",
    "\n",
    "distances_df = pd.DataFrame(list_of_distances, columns=['text1', 'text2', 'distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9039434d-f82d-4c2d-9721-358c84e67369",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text1, text2, distance]\n",
       "Index: []"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances_df[distances_df.text1 == 'world']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5266af-6713-4065-a22c-b929ded61d77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cos_dist = 1 - cosine(target_word_embeddings[0], np.sum(target_word_embeddings, axis=0))\n",
    "print(f'Distance between context-free and context-averaged = {cos_dist}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00de55bd-0bd3-4046-a18d-d36bb7fe3aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "566e85ab-825f-4274-bded-db35ba434c39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('../../../data/processed/light.csv')\n",
    "# Filter\n",
    "timestamps = df.year.to_list()\n",
    "texts = df.text.to_list()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3979516-4c7e-4170-a66c-c7c4a2ff17f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bert_text_preparation(texts, tokenizer, max_seq_length=612):\n",
    "    \"\"\"Preparing the input for BERT\n",
    "    \n",
    "    Takes a list of strings (texts) and performs\n",
    "    pre-processing like adding special tokens,\n",
    "    tokenization, tokens to ids, and tokens to\n",
    "    segment ids. Each sentence is treated as a separate segment.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of strings (sentences/documents) to be converted\n",
    "        tokenizer (obj): Tokenizer object\n",
    "            to convert text into BERT-re-\n",
    "            adable tokens and ids\n",
    "        max_seq_length (int): Maximum sequence length supported by the BERT model\n",
    "        \n",
    "    Returns:\n",
    "        list: List of lists of BERT-readable tokens for each sentence\n",
    "        obj: Torch tensor with token ids\n",
    "        obj: Torch tensor segment ids\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    tokenized_texts = []\n",
    "    tokens_tensors = []\n",
    "    segments_tensors = []\n",
    "\n",
    "    for text in texts:\n",
    "        # Calculate how much to truncate from the beginning and end\n",
    "        truncate_length = len(text) - max_seq_length + 2  # +2 to account for [CLS] and [SEP]\n",
    "\n",
    "        # Truncate the beginning and end of the text\n",
    "        truncated_text = text[truncate_length//2 : -truncate_length//2]\n",
    "\n",
    "        marked_text = \"[CLS] \" + truncated_text + \" [SEP]\"\n",
    "        tokenized_text = tokenizer.tokenize(marked_text)\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        segments_ids = [1] * len(indexed_tokens)\n",
    "\n",
    "        tokenized_texts.append(tokenized_text)\n",
    "        tokens_tensors.append(indexed_tokens)\n",
    "        segments_tensors.append(segments_ids)\n",
    "\n",
    "    # Pad sequences to max_seq_length\n",
    "    tokens_tensors = torch.nn.utils.rnn.pad_sequence([torch.tensor(t) for t in tokens_tensors], batch_first=True)\n",
    "    segments_tensors = torch.nn.utils.rnn.pad_sequence([torch.tensor(s) for s in segments_tensors], batch_first=True)\n",
    "\n",
    "    return tokenized_texts, tokens_tensors, segments_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bb1d46d-0f5b-4db0-b5e8-42c113ec2d18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased', \n",
    "                                  output_hidden_states = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1810ff0e-6f48-4f37-9d72-c8e92bf93a9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare input using the bert_text_preparation function\n",
    "tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(texts, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9243ee-cf77-4b6e-9cdb-18e4655aee8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "    hidden_states = outputs[2]\n",
    "\n",
    "# Selecting the output embeddings from the last layer\n",
    "token_embeddings = hidden_states[-1]\n",
    "\n",
    "# Assuming you want the embeddings for the first word in the first sentence\n",
    "word_index = tokenized_texts[0].index(\"sovereignty\")\n",
    "word_embedding = token_embeddings[0, word_index].numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2868efef-50ed-4a17-88be-a9381d7aebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Assuming you have the BERT model loaded and the embeddings for each word\n",
    "# Example: word_embedding_sovereignty and word_embedding_territorial\n",
    "\n",
    "# Reshape embeddings to be 2D arrays\n",
    "embedding_sovereignty = word_embedding_sovereignty.reshape(1, -1)\n",
    "embedding_territorial = word_embedding_territorial.reshape(1, -1)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_sim = cosine_similarity(embedding_sovereignty, embedding_territorial)\n",
    "\n",
    "print(\"Cosine Similarity between 'sovereignty' and 'territorial integrity':\", cosine_sim[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a069ed-d2c2-4536-a156-730b843106b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Assuming you have the BERT model loaded and the embeddings for each word at different time points\n",
    "# Example: word_embedding_sovereignty_list and word_embedding_territorial_list\n",
    "\n",
    "# Placeholder lists to store distances and time points\n",
    "distances = []\n",
    "time_points = []\n",
    "\n",
    "# Collect embeddings at different time points\n",
    "for time_point in range(num_time_points):\n",
    "    # Example: Obtain word embeddings at different time points\n",
    "    word_embedding_sovereignty = get_word_embedding_at_time_point(\"sovereignty\", time_point)\n",
    "    word_embedding_territorial = get_word_embedding_at_time_point(\"territorial integrity\", time_point)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_sim = cosine_similarity(word_embedding_sovereignty.reshape(1, -1), word_embedding_territorial.reshape(1, -1))\n",
    "\n",
    "    # Append distance and time point\n",
    "    distances.append(cosine_sim[0][0])\n",
    "    time_points.append(time_point)\n",
    "\n",
    "# Create a Plotly figure\n",
    "fig = px.line(x=time_points, y=distances, markers=True, labels={'x': 'Time Points', 'y': 'Cosine Similarity'},\n",
    "              title='Cosine Similarity between \"sovereignty\" and \"territorial integrity\" over Time')\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcf31c4-b215-4b04-a055-31144f01cffc",
   "metadata": {},
   "source": [
    "# Fast Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09a6c347-e5fa-4e0d-827b-7491117afd12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased', \n",
    "                                  output_hidden_states = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a06a6c8-b732-4d94-a4ce-a2c56a6c55cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bert_text_preparation(texts, tokenizer, max_seq_length=512):\n",
    "    tokenized_texts = []\n",
    "    tokens_tensors = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in texts:\n",
    "        # Tokenize the text\n",
    "        tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "        truncate_length = len(text) - max_seq_length + 2  # +2 to account for [CLS] and [SEP]\n",
    "\n",
    "        # Truncate the beginning and end of the text\n",
    "        truncated_text = text[truncate_length//2 : -truncate_length//2]\n",
    "\n",
    "        marked_text = \"[CLS] \" + truncated_text + \" [SEP]\"\n",
    "        # Add special tokens [CLS] and [SEP]\n",
    "        marked_text = [\"[CLS]\"] + tokenized_text + [\"[SEP]\"]\n",
    "\n",
    "        # Convert tokens to ids\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(marked_text)\n",
    "\n",
    "        # Create attention mask\n",
    "        attention_mask = [1] * len(indexed_tokens)\n",
    "\n",
    "        # Pad sequences to max_seq_length\n",
    "        while len(indexed_tokens) < max_seq_length:\n",
    "            indexed_tokens.append(0)\n",
    "            attention_mask.append(0)\n",
    "\n",
    "        # Convert lists to PyTorch tensors\n",
    "        tokens_tensors.append(torch.tensor(indexed_tokens))\n",
    "        attention_masks.append(torch.tensor(attention_mask))\n",
    "\n",
    "        tokenized_texts.append(tokenized_text)\n",
    "\n",
    "    # Convert lists to PyTorch tensors\n",
    "    tokens_tensors = torch.stack(tokens_tensors)\n",
    "    attention_masks = torch.stack(attention_masks)\n",
    "\n",
    "    return tokenized_texts, tokens_tensors, attention_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7801b8c6-2107-4294-9701-63aa6eb48445",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (628 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Prepare input using the bert_text_preparation function\n",
    "tokenized_text, tokens_tensor, attention_masks = bert_text_preparation(texts, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be7b12e-d8bd-42f6-a481-39181b8ebfbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the BERT model\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=tokens_tensor.view(-1, tokens_tensor.size(-1)), attention_mask=attention_masks.view(-1, attention_masks.size(-1)))\n",
    "    \n",
    "\n",
    "# Selecting the output embeddings from the last layer\n",
    "#token_embeddings = hidden_states[-1]\n",
    "\n",
    "# Assuming you want the embeddings for the first word in the first sentence\n",
    "#word_index = tokenized_texts[0].index(\"sovereignty\")\n",
    "#word_embedding = token_embeddings[0, word_index].numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c16efb2-abc0-4346-acc9-ba10aa050010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
